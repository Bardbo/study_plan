{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.查看自己是否有GPU，有就使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.定义nn模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # 对输入进行展开\n",
    "        self.flatten = nn.Flatten()\n",
    "        # 模型的层\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    # 前向传播过程\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten()\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 模型实例化并转移到GPU上，这里如果没有GPU那么仍在CPU上\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([7], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 随机生成一个数据，并将其指定device\n",
    "X = torch.rand(1, 28, 28, device=device)\n",
    "\n",
    "# 前向传播\n",
    "logits = model(X)\n",
    "\n",
    "# 对结果进行softmax\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "# 取得其中最大值的索引当作类别\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.模型的Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# 随机生成一个矩阵（这里注意一下，以前仿照的是单通道的28，28的图片，现在是3通道，也可以视作3条）\n",
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "# flatten的演示\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "# 线性层的演示\n",
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.1020,  0.5016,  0.6345,  0.2780,  0.0573,  0.0249, -0.2846,  0.2273,\n",
      "         -0.1942, -0.2875,  0.4988, -0.0968, -0.2809,  0.5053,  0.0411, -0.1028,\n",
      "         -0.5755,  0.1940,  0.2414,  0.2868],\n",
      "        [ 0.0041,  0.4441,  0.5011,  0.2754,  0.2976, -0.1346, -0.3806,  0.1028,\n",
      "         -0.2003, -0.3094,  0.4199,  0.2170,  0.0357,  0.5989,  0.0412,  0.1393,\n",
      "         -0.2759,  0.2999,  0.0496,  0.4288],\n",
      "        [ 0.1191,  0.2174,  0.5183,  0.0805,  0.0457, -0.2870, -0.2059,  0.1480,\n",
      "         -0.1642,  0.1174,  0.1038, -0.0619,  0.0348,  0.4548,  0.2127,  0.0063,\n",
      "         -0.2326,  0.2660,  0.1505,  0.0909]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.1020, 0.5016, 0.6345, 0.2780, 0.0573, 0.0249, 0.0000, 0.2273, 0.0000,\n",
      "         0.0000, 0.4988, 0.0000, 0.0000, 0.5053, 0.0411, 0.0000, 0.0000, 0.1940,\n",
      "         0.2414, 0.2868],\n",
      "        [0.0041, 0.4441, 0.5011, 0.2754, 0.2976, 0.0000, 0.0000, 0.1028, 0.0000,\n",
      "         0.0000, 0.4199, 0.2170, 0.0357, 0.5989, 0.0412, 0.1393, 0.0000, 0.2999,\n",
      "         0.0496, 0.4288],\n",
      "        [0.1191, 0.2174, 0.5183, 0.0805, 0.0457, 0.0000, 0.0000, 0.1480, 0.0000,\n",
      "         0.1174, 0.1038, 0.0000, 0.0348, 0.4548, 0.2127, 0.0063, 0.0000, 0.2660,\n",
      "         0.1505, 0.0909]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# relu激活的展示\n",
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.Sequential的展示，串联\n",
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1111, 0.1340, 0.1066, 0.0781, 0.0942, 0.1108, 0.1008, 0.0702, 0.0875,\n",
       "          0.1068],\n",
       "         [0.1075, 0.1181, 0.1011, 0.0809, 0.0934, 0.1271, 0.0908, 0.0840, 0.1071,\n",
       "          0.0900],\n",
       "         [0.1204, 0.1230, 0.1048, 0.0875, 0.0837, 0.1365, 0.0821, 0.0715, 0.0878,\n",
       "          0.1025]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax的展示\n",
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)\n",
    "pred_probab, pred_probab.sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.模型的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:  NeuralNetwork(\n",
      "  (flatten): Flatten()\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0082, -0.0298, -0.0238,  ..., -0.0079,  0.0283,  0.0121],\n",
      "        [ 0.0158,  0.0260, -0.0197,  ...,  0.0127, -0.0198,  0.0282]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([ 0.0146, -0.0173], device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0435, -0.0424,  0.0379,  ...,  0.0345, -0.0199,  0.0070],\n",
      "        [ 0.0291, -0.0037, -0.0088,  ..., -0.0296, -0.0237,  0.0267]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0353, -0.0102], device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0115,  0.0379,  0.0187,  ...,  0.0135,  0.0279,  0.0441],\n",
      "        [-0.0239,  0.0286, -0.0335,  ..., -0.0312, -0.0234,  0.0437]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0163, -0.0025], device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Model structure: \", model, \"\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
